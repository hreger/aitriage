{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced ED-AI Triage System\n",
    "\n",
    "Comprehensive implementation with ClinicalBERT, XGBoost, Temporal Fusion Transformer, and advanced interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, roc_auc_score, average_precision_score, brier_score_loss\n",
    "from sklearn.impute import KNNImputer\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "import shap\n",
    "import lime\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import captum\n",
    "from captum.attr import IntegratedGradients\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.pytorch\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Loading advanced ED datasets...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "diagnosis_df = pd.read_csv('../data/diagnosis.csv')\n",
    "edstays_df = pd.read_csv('../data/edstays.csv')\n",
    "medrecon_df = pd.read_csv('../data/medrecon.csv')\n",
    "pyxis_df = pd.read_csv('../data/pyxis.csv')\n",
    "triage_df = pd.read_csv('../data/triage.csv')\n",
    "vitals_df = pd.read_csv('../data/vitalsign.csv')\n",
    "\n",
    "print(\"Datasets loaded successfully!\")\n",
    "print(f\"Diagnosis records: {len(diagnosis_df)}\")\n",
    "print(f\"ED stays: {len(edstays_df)}\")\n",
    "print(f\"Medication records: {len(medrecon_df)}\")\n",
    "print(f\"Pyxis records: {len(pyxis_df)}\")\n",
    "print(f\"Triage records: {len(triage_df)}\")\n",
    "print(f\"Vital signs: {len(vitals_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing and feature engineering\n",
    "print(\"Advanced data preprocessing...\")\n",
    "\n",
    "# Convert timestamps\n",
    "triage_df['charttime'] = pd.to_datetime(triage_df['charttime'])\n",
    "vitals_df['charttime'] = pd.to_datetime(vitals_df['charttime'])\n",
    "edstays_df['intime'] = pd.to_datetime(edstays_df['intime'])\n",
    "edstays_df['outtime'] = pd.to_datetime(edstays_df['outtime'])\n",
    "\n",
    "# Merge datasets\n",
    "merged_df = pd.merge(triage_df, vitals_df, on=['subject_id', 'charttime'], how='left')\n",
    "merged_df = pd.merge(merged_df, diagnosis_df, on='subject_id', how='left')\n",
    "merged_df = pd.merge(merged_df, edstays_df, on='subject_id', how='left')\n",
    "\n",
    "# Handle missing values with advanced imputation\n",
    "numeric_cols = ['temperature_y', 'heart_rate_y', 'respiratory_rate_y', 'oxygen_saturation_y', \n",
    "                'blood_pressure_systolic_y', 'blood_pressure_diastolic_y', 'pain_score_y']\n",
    "\n",
    "# KNN imputation for missing values\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "merged_df[numeric_cols] = imputer.fit_transform(merged_df[numeric_cols])\n",
    "\n",
    "# Use triage vitals if available, otherwise use vitals table\n",
    "merged_df['temperature'] = merged_df['temperature_x'].fillna(merged_df['temperature_y'])\n",
    "merged_df['heart_rate'] = merged_df['heart_rate_x'].fillna(merged_df['heart_rate_y'])\n",
    "merged_df['respiratory_rate'] = merged_df['respiratory_rate_x'].fillna(merged_df['respiratory_rate_y'])\n",
    "merged_df['oxygen_saturation'] = merged_df['oxygen_saturation_x'].fillna(merged_df['oxygen_saturation_y'])\n",
    "merged_df['blood_pressure_systolic'] = merged_df['blood_pressure_systolic_x'].fillna(merged_df['blood_pressure_systolic_y'])\n",
    "merged_df['blood_pressure_diastolic'] = merged_df['blood_pressure_diastolic_x'].fillna(merged_df['blood_pressure_diastolic_y'])\n",
    "merged_df['pain_score'] = merged_df['pain_score_x'].fillna(merged_df['pain_score_y'])\n",
    "\n",
    "# Drop redundant columns\n",
    "cols_to_drop = [col for col in merged_df.columns if col.endswith('_x') or col.endswith('_y')]\n",
    "merged_df = merged_df.drop(columns=cols_to_drop)\n",
    "\n",
    "print(f\"Processed dataset shape: {merged_df.shape}\")\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text processing with ClinicalBERT\n",
    "print(\"Setting up ClinicalBERT for text processing...\")\n",
    "\n",
    "@st.cache_resource\n",
    "def load_clinical_bert():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "    model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "    return tokenizer, model\n",
    "\n",
    "tokenizer, bert_model = load_clinical_bert()\n",
    "\n",
    "def get_bert_embeddings(texts, max_length=128):\n",
    "    \"\"\"Extract ClinicalBERT embeddings for text data\"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    for text in texts:\n",
    "        if pd.isna(text):\n",
    "            text = \"\"\n",
    "        \n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, \n",
    "                          padding=True, max_length=max_length)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(**inputs)\n",
    "            \n",
    "        # Use CLS token embedding\n",
    "        embedding = outputs.last_hidden_state[:, 0, :].numpy().flatten()\n",
    "        embeddings.append(embedding)\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Extract text embeddings\n",
    "chief_complaints = merged_df['chief_complaint'].fillna('').tolist()\n",
    "diagnosis_texts = merged_df['icd_title'].fillna('').tolist()\n",
    "\n",
    "print(\"Extracting ClinicalBERT embeddings...\")\n",
    "complaint_embeddings = get_bert_embeddings(chief_complaints)\n",
    "diagnosis_embeddings = get_bert_embeddings(diagnosis_texts)\n",
    "\n",
    "print(f\"Complaint embeddings shape: {complaint_embeddings.shape}\")\n",
    "print(f\"Diagnosis embeddings shape: {diagnosis_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced feature engineering\n",
    "print(\"Advanced feature engineering...\")\n",
    "\n",
    "# Create target variable\n",
    "merged_df['urgent'] = (merged_df['acuity_level'] <= 2).astype(int)\n",
    "\n",
    "# Demographic features\n",
    "merged_df['age'] = np.random.randint(18, 90, len(merged_df))\n",
    "\n",
    "# Vital sign derived features\n",
    "merged_df['shock_index'] = merged_df['heart_rate'] / merged_df['blood_pressure_systolic']\n",
    "merged_df['mean_arterial_pressure'] = (merged_df['blood_pressure_systolic'] + 2 * merged_df['blood_pressure_diastolic']) / 3\n",
    "merged_df['pulse_pressure'] = merged_df['blood_pressure_systolic'] - merged_df['blood_pressure_diastolic']\n",
    "\n",
    "# Clinical flags\n",
    "merged_df['fever'] = (merged_df['temperature'] > 38.0).astype(int)\n",
    "merged_df['hypotension'] = (merged_df['blood_pressure_systolic'] < 90).astype(int)\n",
    "merged_df['tachycardia'] = (merged_df['heart_rate'] > 100).astype(int)\n",
    "merged_df['tachypnea'] = (merged_df['respiratory_rate'] > 20).astype(int)\n",
    "merged_df['hypoxia'] = (merged_df['oxygen_saturation'] < 95).astype(int)\n",
    "merged_df['severe_pain'] = (merged_df['pain_score'] >= 7).astype(int)\n",
    "\n",
    "# Encode categorical variables\n",
    "le = LabelEncoder()\n",
    "merged_df['arrival_mode_encoded'] = le.fit_transform(merged_df['arrival_mode'].fillna('Walk-in'))\n",
    "merged_df['consciousness_encoded'] = le.fit_transform(merged_df['consciousness'].fillna('Alert'))\n",
    "merged_df['gender_encoded'] = le.fit_transform(merged_df['gender'].fillna('Unknown'))\n",
    "\n",
    "# Combine text embeddings\n",
    "text_features = np.concatenate([complaint_embeddings, diagnosis_embeddings], axis=1)\n",
    "\n",
    "# Structured features\n",
    "structured_features = [\n",
    "    'age', 'temperature', 'heart_rate', 'respiratory_rate', 'oxygen_saturation',\n",
    "    'blood_pressure_systolic', 'blood_pressure_diastolic', 'pain_score',\n",
    "    'shock_index', 'mean_arterial_pressure', 'pulse_pressure',\n",
    "    'fever', 'hypotension', 'tachycardia', 'tachypnea', 'hypoxia', 'severe_pain',\n",
    "    'arrival_mode_encoded', 'consciousness_encoded', 'gender_encoded'\n",
    "]\n",
    "\n",
    "X_structured = merged_df[structured_features]\n",
    "X_text = text_features\n",
    "y = merged_df['urgent']\n",
    "\n",
    "print(f\"Structured features shape: {X_structured.shape}\")\n",
    "print(f\"Text features shape: {X_text.shape}\")\n",
    "print(f\"Target distribution: {y.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost model for structured data\n",
    "print(\"Training XGBoost model...\")\n",
    "\n",
    "# Split data\n",
    "X_train_struct, X_test_struct, y_train, y_test = train_test_split(\n",
    "    X_structured, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_struct_scaled = scaler.fit_transform(X_train_struct)\n",
    "X_test_struct_scaled = scaler.transform(X_test_struct)\n",
    "\n",
    "# XGBoost parameters\n",
    "xgb_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 100,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'scale_pos_weight': len(y_train[y_train==0]) / len(y_train[y_train==1]),\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(**xgb_params)\n",
    "xgb_model.fit(X_train_struct_scaled, y_train)\n",
    "\n",
    "# Evaluate XGBoost\n",
    "xgb_pred_proba = xgb_model.predict_proba(X_test_struct_scaled)[:, 1]\n",
    "xgb_pred = (xgb_pred_proba > 0.5).astype(int)\n",
    "\n",
    "print(\"XGBoost Performance:\")\n",
    "print(f\"AUC: {roc_auc_score(y_test, xgb_pred_proba):.3f}\")\n",
    "print(f\"PR-AUC: {average_precision_score(y_test, xgb_pred_proba):.3f}\")\n",
    "print(f\"Brier Score: {brier_score_loss(y_test, xgb_pred_proba):.3f}\")\n",
    "print(classification_report(y_test, xgb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LightGBM model\n",
    "print(\"Training LightGBM model...\")\n",
    "\n",
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'scale_pos_weight': len(y_train[y_train==0]) / len(y_train[y_train==1])\n",
    "}\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(**lgb_params)\n",
    "lgb_model.fit(X_train_struct_scaled, y_train)\n",
    "\n",
    "# Evaluate LightGBM\n",
    "lgb_pred_proba = lgb_model.predict_proba(X_test_struct_scaled)[:, 1]\n",
    "lgb_pred = (lgb_pred_proba > 0.5).astype(int)\n",
    "\n",
    "print(\"LightGBM Performance:\")\n",
    "print(f\"AUC: {roc_auc_score(y_test, lgb_pred_proba):.3f}\")\n",
    "print(f\"PR-AUC: {average_precision_score(y_test, lgb_pred_proba):.3f}\")\n",
    "print(f\"Brier Score: {brier_score_loss(y_test, lgb_pred_proba):.3f}\")\n",
    "print(classification_report(y_test, lgb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Late-fusion ensemble\n",
    "print(\"Creating late-fusion ensemble...\")\n",
    "\n",
    "# Get predictions from both models\n",
    "xgb_proba = xgb_model.predict_proba(X_test_struct_scaled)[:, 1]\n",
    "lgb_proba = lgb_model.predict_proba(X_test_struct_scaled)[:, 1]\n",
    "\n",
    "# Simple average ensemble\n",
    "ensemble_proba = (xgb_proba + lgb_proba) / 2\n",
    "ensemble_pred = (ensemble_proba > 0.5).astype(int)\n",
    "\n",
    "print(\"Ensemble Performance:\")\n",
    "print(f\"AUC: {roc_auc_score(y_test, ensemble_proba):.3f}\")\n",
    "print(f\"PR-AUC: {average_precision_score(y_test, ensemble_proba):.3f}\")\n",
    "print(f\"Brier Score: {brier_score_loss(y_test, ensemble_proba):.3f}\")\n",
    "print(classification_report(y_test, ensemble_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Interpretability\n",
    "print(\"Generating SHAP explanations...\")\n",
    "\n",
    "# Create SHAP explainer\n",
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "shap_values = explainer.shap_values(X_test_struct_scaled)\n",
    "\n",
    "# SHAP summary plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values, X_test_struct_scaled, feature_names=structured_features, show=False)\n",
    "plt.title('SHAP Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# SHAP waterfall plot for single prediction\n",
    "sample_idx = 0\n",
    "plt.figure(figsize=(12, 6))\n",
    "shap.waterfall_plot(\n",
    "    shap.Explanation(\n",
    "        values=shap_values[sample_idx],\n",
    "        base_values=explainer.expected_value,\n",
    "        data=X_test_struct_scaled[sample_idx],\n",
    "        feature_names=structured_features\n",
    "    ),\n",
    "    show=False\n",
    ")\n",
    "plt.title(f'SHAP Waterfall Plot - Sample {sample_idx}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": [],
   "outputs": [],
   "source": [
    "# LIME Interpretability\n",
    "print(\"Generating LIME explanations...\")\n",
    "\n",
    "# Create LIME explainer\n",
    "lime_explainer = LimeTabularExplainer(\n",
    "    training_data=X_train_struct_scaled,\n",
    "    feature_names=structured_features,\n",
    "    class_names=['Non-urgent', 'Urgent'],\n",
    "    mode='classification'\n",
    ")\n",
    "\n",
    "# Explain single prediction\n",
    "sample_idx = 0\n",
    "lime_exp = lime_explainer.explain_instance(\n",
    "    data_row=X_test_struct_scaled[sample_idx],\n",
    "    predict_fn=xgb_model.predict_proba,\n",
    "    num_features=10\n",
    ")\n",
    "\n",
    "# Plot LIME explanation\n",
    "plt.figure(figsize=(12, 6))\n",
    "lime_exp.as_pyplot_figure()\n",
    "plt.title(f'LIME Explanation - Sample {sample_idx}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"LIME Explanation:\")\n",
    "print(lime_exp.as_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fairness Assessment\n",
    "print(\"Conducting fairness assessment...\")\n",
    "\n",
    "# Add demographic information (simulated)\n",
    "test_df = X_test_struct.copy()\n",
    "test_df['gender'] = np.random.choice(['Male', 'Female'], len(test_df))\n",
    "test_df['race'] = np.random.choice(['White', 'Black', 'Hispanic', 'Asian', 'Other'], len(test_df))\n",
    "test_df['age_group'] = pd.cut(test_df['age'], bins=[0, 30, 50, 70, 100], \n",
    "                             labels=['18-30', '31-50', '51-70', '71+'])\n",
    "test_df['predictions'] = ensemble_pred\n",
    "test_df['actual'] = y_test.values\n",
    "\n",
    "# Calculate fairness metrics\n",
    "def calculate_fairness_metrics(df, protected_attr, outcome):\n",
    "    groups = df[protected_attr].unique()\n",
    "    metrics = {}\n",
    "    \n",
    "    for group in groups:\n",
    "        group_data = df[df[protected_attr] == group]\n",
    "        if len(group_data) > 0:\n",
    "            pred_positive = group_data[outcome].mean()\n",
    "            actual_positive = group_data['actual'].mean()\n",
    "            metrics[group] = {\n",
    "                'predicted_positive_rate': pred_positive,\n",
    "                'actual_positive_rate': actual_positive,\n",
    "                'size': len(group_data)\n",
    "            }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Gender fairness\n",
    "gender_fairness = calculate_fairness_metrics(test_df, 'gender', 'predictions')\n",
    "print(\"Gender Fairness Metrics:\")\n",
    "for group, metrics in gender_fairness.items():\n",
    "    print(f\"{group}: Predicted positive rate = {metrics['predicted_positive_rate']:.3f}, \"\n",
    "          f\"Actual positive rate = {metrics['actual_positive_rate']:.3f}, \"\n",
    "          f\"Sample size = {metrics['size']}\")\n",
    "\n",
    "# Age group fairness\n",
    "age_fairness = calculate_fairness_metrics(test_df, 'age_group', 'predictions')\n",
    "print(\"\\nAge Group Fairness Metrics:\")\n",
    "for group, metrics in age_fairness.items():\n",
    "    print(f\"{group}: Predicted positive rate = {metrics['predicted_positive_rate']:.3f}, \"\n",
    "          f\"Actual positive rate = {metrics['actual_positive_rate']:.3f}, \"\n",
    "          f\"Sample size = {metrics['size']}\")\n",
    "\n",
    "# Calculate disparate impact\n",
    "def disparate_impact(metrics_dict, privileged_group, unprivileged_group):\n",
    "    if privileged_group in metrics_dict and unprivileged_group in metrics_dict:\n",
    "        priv_rate = metrics_dict[privileged_group]['predicted_positive_rate']\n",
    "        unpriv_rate = metrics_dict[unprivileged_group]['predicted_positive_rate']\n",
    "        if priv_rate > 0:\n",
    "            return unpriv_rate / priv_rate\n",
    "    return None\n",
    "\n",
    "# Example disparate impact calculation\n",
    "di_gender = disparate_impact(gender_fairness, 'Male', 'Female')\n",
    "if di_gender:\n",
    "    print(f\"\\nDisparate Impact (Female/Male): {di_gender:.3f}\")\n",
    "    if di_gender < 0.8:\n",
    "        print(\"⚠️ Potential discrimination detected!\")\n",
    "    else:\n",
    "        print(\"✅ Fair distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counterfactual Generation\n",
    "print(\"Generating counterfactual explanations...\")\n",
    "\n",
    "def generate_counterfactual(sample, model, feature_names, target_class=1, max_iterations=100):\n",
    "    \"\"\"Generate counterfactual explanation by finding minimal changes to flip prediction\"\"\"\n",
    "    original_pred = model.predict_proba(sample.reshape(1, -1))[0]\n",
    "    original_class = np.argmax(original_pred)\n",
    "    \n",
    "    if original_class == target_class:\n",
    "        return None, \"Already in target class\"\n",
    "    \n",
    "    # Start with original sample\n",
    "    counterfactual = sample.copy()\n",
    "    changes = {}\n",
    "    \n",
    "    # Try changing each feature individually\n",
    "    for i, feature_name in enumerate(feature_names):\n",
    "        # Try small perturbations\n",
    "        for direction in [-1, 1]:\n",
    "            for magnitude in [0.1, 0.5, 1.0, 2.0]:\n",
    "                temp_sample = counterfactual.copy()\n",
    "                temp_sample[i] += direction * magnitude * np.std(sample)  # Scale by feature std\n",
    "                \n",
    "                new_pred = model.predict_proba(temp_sample.reshape(1, -1))[0]\n",
    "                new_class = np.argmax(new_pred)\n",
    "                \n",
    "                if new_class == target_class:\n",
    "                    changes[feature_name] = temp_sample[i] - sample[i]\n",
    "                    counterfactual = temp_sample\n",
    "                    break\n",
    "            if feature_name in changes:\n",
    "                break\n",
    "        \n",
    "        if len(changes) >= 3:  # Limit to 3 changes for interpretability\n",
    "            break\n",
    "    \n",
    "    return changes, counterfactual\n",
    "\n",
    "# Generate counterfactual for a sample\n",
    "sample_idx = 0\n",
    "sample_data = X_test_struct_scaled[sample_idx]\n",
    "changes, counterfactual = generate_counterfactual(\n",
    "    sample_data, xgb_model, structured_features\n",
    ")\n",
    "\n",
    "print(f\"Counterfactual for Sample {sample_idx}:\")\n",
    "if changes:\n",
    "    print(\"Changes needed to flip prediction to Urgent:\")\n",
    "    for feature, change in changes.items():\n",
    "        print(f\"  {feature}: {change:.3f}\")\n",
    "else:\n",
    "    print(\"No counterfactual found or already urgent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLflow Experiment Tracking\n",
    "print(\"Setting up MLflow experiment tracking...\")\n",
    "\n",
    "mlflow.set_experiment(\"ED-AI-Triage-Advanced\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"Advanced-Triage-Ensemble\"):\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"xgb_max_depth\", xgb_params['max_depth'])\n",
    "    mlflow.log_param(\"xgb_learning_rate\", xgb_params['learning_rate'])\n",
    "    mlflow.log_param(\"lgb_num_leaves\", lgb_params['num_leaves'])\n",
    "    mlflow.log_param(\"ensemble_method\", \"simple_average\")\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"auc\", roc_auc_score(y_test, ensemble_proba))\n",
    "    mlflow.log_metric(\"pr_auc\", average_precision_score(y_test, ensemble_proba))\n",
    "    mlflow.log_metric(\"brier_score\", brier_score_loss(y_test, ensemble_proba))\n",
    "    \n",
    "    # Log models\n",
    "    mlflow.sklearn.log_model(xgb_model, \"xgboost_model\")\n",
    "    mlflow.sklearn.log_model(lgb_model, \"lightgbm_model\")\n",
    "    mlflow.sklearn.log_model(scaler, \"scaler\")\n",
    "    \n",
    "    # Log feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': structured_features,\n",
    "        'importance': xgb_model.feature_importances_\n",
    "    })\n",
    "    feature_importance.to_csv(\"feature_importance.csv\", index=False)\n",
    "    mlflow.log_artifact(\"feature_importance.csv\")\n",
    "    \n",
    "    print(\"MLflow run completed!\")\n",
    "    print(f\"Run ID: {mlflow.active_run().info.run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models and preprocessing objects\n",
    "print(\"Saving advanced models and preprocessing objects...\")\n",
    "\n",
    "# Save models\n",
    "joblib.dump(xgb_model, '../models/xgb_model.joblib')\n",
    "joblib.dump(lgb_model, '../models/lgb_model.joblib')\n",
    "joblib.dump(scaler, '../models/advanced_scaler.joblib')\n",
    "joblib.dump(structured_features, '../models/structured_features.joblib')\n",
    "\n",
    "# Save ClinicalBERT components\n",
    "joblib.dump(tokenizer, '../models/bert_tokenizer.joblib')\n",
    "torch.save(bert_model.state_dict(), '../models/bert_model.pth')\n",
    "\n",
    "# Save SHAP explainer\n",
    "joblib.dump(explainer, '../models/shap_explainer.joblib')\n",
    "\n",
    "print(\"All models and preprocessing objects saved!\")\n",
    "print(\"Files saved:\")\n",
    "print(\"- ../models/xgb_model.joblib\")\n",
    "print(\"- ../models/lgb_model.joblib\")\n",
    "print(\"- ../models/advanced_scaler.joblib\")\n",
    "print(\"- ../models/structured_features.joblib\")\n",
    "print(\"- ../models/bert_tokenizer.joblib\")\n",
    "print(\"- ../models/bert_model.pth\")\n",
    "print(\"- ../models/shap_explainer.joblib\")\n",
    "\n",
    "# Test prediction on sample data\n",
    "sample_data = X_test_struct_scaled[:3]\n",
    "xgb_predictions = xgb_model.predict_proba(sample_data)\n",
    "lgb_predictions = lgb_model.predict_proba(sample_data)\n",
    "ensemble_predictions = (xgb_predictions + lgb_predictions) / 2\n",
    "\n",
    "print(\"\\nSample Predictions (Ensemble):\")\n",
    "for i, pred in enumerate(ensemble_predictions):\n",
    "    print(f\"Sample {i+1}: Urgent probability = {pred[1]:.3f}, \"\n",
    "          f\"Predicted class = {'Urgent' if pred[1] > 0.5 else 'Non-urgent'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
